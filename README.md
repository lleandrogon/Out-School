# üìä ETL Out-of-School Pipeline  
Pipeline de ETL usando **Apache Airflow**, **Pandas** e **PostgreSQL** para processar dados p√∫blicos sobre pessoas fora da escola (Fundamental I, Fundamental II e Ensino M√©dio).

## üöÄ Vis√£o Geral

Este projeto implementa um fluxo de ETL totalmente automatizado utilizando **Apache Airflow**.  
O pipeline:

1. **Extrai** dados brutos de tr√™s arquivos CSV:
   - primary.csv  
   - Lower Secondary.csv  
   - Upper Secondary.csv

2. **Transforma** os dados com **Pandas**, padronizando colunas, ajustando tipos, lidando com valores nulos e preparando tudo para armazenamento.

3. **Carrega** os dados transformados em um banco de dados **PostgreSQL**.

Todo o ambiente √© executado atrav√©s de **Docker Compose**, usando a imagem oficial do Airflow.

---

**Tecnologias utilizadas:**

- Apache Airflow  
- Python 3 + Pandas  
- Docker Compose  
- PostgreSQL

---

## üê≥ Como executar o projeto

### 1Ô∏è‚É£ Pr√©-requisitos

- Docker  
- Docker Compose  

### 2Ô∏è‚É£ Subir o ambiente Airflow

No diret√≥rio raiz do projeto, execute:

**docker compose up -d**

### 3Ô∏è‚É£ Configurar a conex√£o com o PostgreSQL no Airflow

Para que o Airflow consiga enviar os dados transformados para o PostgreSQL da sua m√°quina local, √© necess√°rio **criar uma conex√£o manualmente na interface web**.

#### üõ†Ô∏è Criando a conex√£o `education_docker`

1. Acesse o Airflow em:  
   **http://localhost:8080**

2. V√° at√© o menu:  
   **Admin ‚Üí Connections**

3. Clique em **+ Add Connection**

4. Preencha os campos da seguinte forma:

| Campo | Valor |
|-------|-------|
| **Conn Id** | `education_docker` |
| **Conn Type** | `Postgres` |
| **Host** | `host.docker.internal` |
| **Schema** | *nome do seu banco local* |
| **Login** | *seu usu√°rio do Postgres* |
| **Password** | *sua senha* |
| **Port** | `5432` |

Essa conex√£o ser√° usada pelo **PostgresHook** dentro das tasks da DAG.

> **Importante:** `host.docker.internal` √© necess√°rio para que o container do Airflow consiga enxergar o PostgreSQL que est√° rodando na sua m√°quina.

---

Depois dessa etapa, seu pipeline j√° estar√° pronto para extrair, transformar e carregar os dados no banco.

Se quiser, posso revisar o README completo e reorganizar para ficar ainda mais profissional.